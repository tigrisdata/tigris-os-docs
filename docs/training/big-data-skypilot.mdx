# Training with Big Data on SkyPilot

SkyPilot simplifies multi-cloud deployments, offering unmatched flexibility for
training large-scale data models across different cloud providers. Unlike custom
solutions or complex tools like Terraform, SkyPilot provides an effortless way
to deploy workloads seamlessly across clouds, eliminating the need for manual
intervention.

With SkyPilot, switching between cloud environments is as easy as updating your
credentials. It abstracts away complexities like egress costs, performance
tuning, and cloud-specific quirks, ensuring a consistent user experience no
matter the provider. By focusing on ergonomic multi-cloud operations, SkyPilot
empowers teams to concentrate on their data workloads instead of the
infrastructure.

In synergy with SkyPilot, Tigris enhances this experience by providing global
object storage that performs consistently across any cloud. Whether handling
large datasets or ensuring low-latency access, Tigris ensures uniform
performance and reliability. Together, SkyPilot and Tigris create a seamless
ecosystem for multi-cloud data operations, enabling fast, reliable storage and
deployment for big data training.

(Link back to blogpost?)

## Prerequisites

- A Tigris account
- [SkyPilot](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html)
  installed
- A bucket you want to use for training and model data (`mybucket`)
- Our
  [demo training repository](https://github.com/tigrisdata-community/skypilot-training-demo)
  cloned
- Accounts with your desired cloud providers (AWS, Azure, GCP, etc; we'll use
  AWS in this guide)

## Setting up your enviroment variables

Copy `.env.example` to `.env` in the root of the training repository.

```text
cp .env.example .env
```

Fill in the following variables:

| Variable                | Description                                                                        | Example                  |
| ----------------------- | ---------------------------------------------------------------------------------- | ------------------------ |
| `AWS_ACCESS_KEY_ID`     | Your Tigris access key                                                             | `tid_*`                  |
| `AWS_SECRET_ACCESS_KEY` | Your Tigris secret key                                                             | `tsec_*`                 |
| `BUCKET_NAME`           | The name of the bucket you want to use for storing model weights and training data | `mybucket`               |
| `DATASET_NAME`          | The name of the dataset you want to use for training                               | `mlabonne/FineTome-100k` |
| `MODEL_NAME`            | The name of the model you want to train                                            | `Qwen/Qwen2.5-0.5B`      |

## Performing training

Kick off training with the following command:

```text
sky launch -n test allatonce.yaml --env-file .env --workdir . -i 15 --down
```

While that runs, here's what's going on:

- A new cloud instance is being created somewhere with a GPU powerful enough to
  train a LoRA adapter on the model you chose in your `.env` file.
- The instance is being provisioned with the necessary software and dependencies
  ([geesefs](https://github.com/yandex-cloud/geesefs) for model storage, the
  `datasets` library in Python, and [Unsloth](https://docs.unsloth.ai/) for
  training).
- When the instance is ready, these steps happen:
  - `import-dataset.py`: The dataset is downloaded from Hugging Face and copied
    to Tigris in shards of up to 5 million examples.
  - `import-dataset.py`: Each shard of the dataset is then standardized so that
    everything is in the same format and can be understood by the model's prior
    instruction tuning.
  - `import-model.py`: The model weights are downloaded from Hugging Face and
    then copied to Tigris for permanent storage.
  - `pretokenize.py`: Each shard of the dataset is loaded from Tigris and then
    uses the model's tokenization formatting to pre-chew it for training. This
    makes sure that all of the data is in the same format and can be understood
    by the model.
  - `dotrain.py`: The model is then trained on each shard of the dataset for one
    epoch and the resulting weights are saved to Tigris.
- Once all of that is done, the instance automatically shuts down after 15
  minutes of inactivity. You can change this value with your argument to
  [the `-i` flag](https://skypilot.readthedocs.io/en/latest/reference/auto-stop.html).

Each of the scripts that run in the instance are designed to be idempotent and
are intended to run in sequence, but `import-dataset.py` and `import-model.py`
can be run in parallel.

(Diagram here)

## Next steps

Once training is complete, you can use the model weights with
[`sky serve`](https://skypilot.readthedocs.io/en/latest/serving/sky-serve.html):

```text
sky serve up -n mymodel service.yaml --env-file .env
```

It will give you a URL that you can use to interact with your model:

```text
curl 3.84.15.251:30001/v1/chat/completions \
    -X POST \
    -d '{"model": "/home/ubuntu/tigris/done/Qwen/Qwen2.5-0.5B/mlabonne/FineTome-100k/fused", "messages": [{"role": "user", "content": "Who are you?"}]}' \
    -H 'Content-Type: application/json'
```

You can destroy this with `sky serve down`:

```text
sky serve down mymodel
```
