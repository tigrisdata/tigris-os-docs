# Training with Big Data on SkyPilot

[SkyPilot](https://skypilot.readthedocs.io/en/latest/docs/index.html) simplifies
multi-cloud deployments, offering flexibility for training large-scale data
models across different cloud providers. Unlike custom solutions or more
involved tools like Terraform, SkyPilot provides an effortless way to deploy
workloads seamlessly across clouds, eliminating the need for manual
intervention.

While multi-cloud tools like SkyPilot abstract away cloud provider specific
quirks, underlying egress costs and bottlenecks in the storage layer can be
roadblocks. Combining Tigris with SkyPilot ensures there are no cross-cloud
egress costs, all while ensuring consistent performance and reliability in any
region.

In this guide, you'll build a multi-cloud compatible model training job that
leverages Tigris to store training data and SkyPilot to manage compute.

## Prerequisites

- A Tigris account
- [SkyPilot](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html)
  installed
- A bucket you want to use for training and model data (`mybucket`)
- Our
  [demo training repository](https://github.com/tigrisdata-community/skypilot-training-demo)
  cloned
- Accounts with your desired cloud providers (AWS, Azure, GCP, etc; we'll use
  AWS in this guide)

## Setting up your enviroment variables

Copy `.env.example` to `.env` in the root of the training repository.

```text
cp .env.example .env
```

Fill in the following variables:

| Variable                | Description                                                                        | Example                  |
| ----------------------- | ---------------------------------------------------------------------------------- | ------------------------ |
| `AWS_ACCESS_KEY_ID`     | Your Tigris access key                                                             | `tid_*`                  |
| `AWS_SECRET_ACCESS_KEY` | Your Tigris secret key                                                             | `tsec_*`                 |
| `BUCKET_NAME`           | The name of the bucket you want to use for storing model weights and training data | `mybucket`               |
| `DATASET_NAME`          | The name of the dataset you want to use for training                               | `mlabonne/FineTome-100k` |
| `MODEL_NAME`            | The name of the model you want to train                                            | `Qwen/Qwen2.5-0.5B`      |

## Performing training

Kick off training with the following command:

```text
sky launch -n test allatonce.yaml --env-file .env --workdir . -i 15 --down
```

## What's happening under the hood?

While that runs, here's what's going on. SkyPilot is spinning up a new GPU
instance somewhere in the cloud, installing the necessary software and
dependencies, and training a LoRA adapter on the model you chose in your `.env`
file. Once training is complete, the instance automatically shuts down after 15
minutes of inactivity.

It does this by running a series of scripts in sequence:

1. `import-dataset.py`: Downloads the dataset from Hugging Face and copies it to
   Tigris in shards of up to 5 million examples. Each shard is saved to Tigris
   unmodified, then standardized so that the model can understand it.
2. `import-model.py`: Downloads the model weights from Hugging Face and copies
   them to Tigris for permanent storage.
3. `pretokenize.py`: Loads each shard of the dataset from Tigris and uses the
   model's tokenization formatting to pre-chew it for training.
4. `dotrain.py`: Trains the model on each shard of the dataset for one epoch and
   saves the resulting weights to Tigris.

Here's a diagram of the process:

![Training pipeline diagram](./overall-flow.webp)

This looks a lot more complicated than it is. Each script is designed to be run
in sequence and everything adds up to a training run that takes about 15 minutes
to complete from start to finish.

Here's some of the most relevant code from each script. You can find the full
scripts in the
[example repo](https://github.com/tigrisdata-community/skypilot-training-demo),
but for these snippets assume the following variables exist and have values that
make sense as relevant:

```python
bucket_name = "mybucket"
dataset_name = "mlabonne/FineTome-100k"
model_name = "Qwen/Qwen2.5-0.5B"
storage_options = { secrets_from_getenv }
```

### Importing and sharding the dataset

One of the biggest things that's hard with managing training data with AI stuff
is dealing with data that is larger than ram. Most of the time when you load a
dataset with the
[`load_dataset` function](https://huggingface.co/docs/datasets/loading), it
downloads all of the data files to the disk and then loads them directly into
memory. This is generally useful for many things, but it means that your dataset
has to be smaller than your memory capacity (minus what your OS needs to exist).

This example works around this problem by using the `streaming=True` option in
`load_dataset`:

```python
dataset = load_dataset(dataset_name, split="train", streaming=True)
```

However, doing this presents additional problems, because when you pass
`streaming=True`, you don't get access to the `save_to_disk` method that you
would normally use to save the dataset to disk. Instead, you have to iterate
over the dataset in chunks and save each
[shard](<https://en.wikipedia.org/wiki/Shard_(database_architecture)>) to disk
manually:

```python
for shard in dataset.iter(5_000_000):
    dataset = Dataset.from_dict(shard, features=dataset.features)
    ds.save_to_disk(f"s3://{bucket_name}/raw/{dataset_name}/{shard_id}", storage_options=storage_options)
```

It's worth noting that when you iterate through things like this, each `shard`
value has all of the data for that shard in CPU ram. You may need to adjust the
shard size to fit your memory capacity. Experimentation is required, but 5
million examples is a good starting point.

Once you have the dataset saved to disk, you have to standardize it because the
data can and will come in whatever format the dataset author thought was
reasonable:

```python
ds = standardize_dataset(ds)
```

After that, you can save the standardized dataset to Tigris:

```python
ds.save_to_disk(f"s3://{bucket_name}/standardized/{dataset_name}/{shard_id}", storage_options=storage_options)
```

There's some additional accounting that's done to keep track of the biggest
shard ID, but that's the gist of it. You take a dataset that can be too big to
fit into memory, break it into chunks that can fit into memory, save them to
Tigris, standardize them, and then save the standardized form to Tigris.

:::note

In very large scale deployments, you may want to move the standardization step
into another discrete stage of this pipeline. Refactoring this into the setup is
therefore trivial and thus left as an exercise for the reader.

:::

### Importing the model

Once all of the data is imported, the model weights are fetched from Hugging
Face's CDN and then written to Tigris:

```python
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = 4096,
    dtype = None, # auto detection
    load_in_4bit = False, # If True, use 4bit quantization to reduce memory usage
)

# Save the model to Tigris
model.save_pretrained(f"{home_dir}/tigris/models/{model_name}")
tokenizer.save_pretrained(f"{home_dir}/tigris/models/{model_name}")
```

You may be asking yourself something like "Wait, why are you saving it to the
disk? I thought it was going to Tigris, Tigris isn't a filesystem, it's object
storage". You're right. Tigris normally is not a filesystem, but S3 (the
technology/API signature that Tigris is built upon) has many semantics that can
somewhat-cleanly-if-you-squint-enough be mapped to the normal Unix filesystem
calls. In order to fit the square peg of Tigris into the round hole of the POSIX
filesystem interface, we use a tool called
[geesefs](https://github.com/yandex-cloud/geesefs). GeeseFS uses the Linux
Kernel's [FUSE](https://www.kernel.org/doc/html/latest/filesystems/fuse.html)
(Filesystem in USErspace) subsystem to mount an S3 bucket as if it was a
filesystem. This allows you to use the normal filesystem calls to interact with
Tigris, which is very useful for AI workloads that assume data is accessible via
a filesystem. We have to do this because the Hugging Face transformers library
assumes that the model weights are on the filesystem, not in object storage.

The model has two main components that we care about:

- The floating-point model weights themselves, these are the weights that get
  loaded into GPU ram and used to inference results based on input data.
- The tokenizer model for that model in particular. This is used to convert text
  into the format that the model understands beneath the hood. Among other
  things, this also provides a string templating function that turns chat
  messages into the raw token form that the model was trained on.

Both of those are stored in Tigris for later use.

### Pre-chewing the dataset

Once the model is saved into Tigris, we have up to `n` shards of up to `m`
examples per shard. All of these examples are in a standard-ish format, but
still needs to be pre-chewed so that training can be super efficient. This is
done by loading each shard from Tigris, using the model's chat tokenization
formatting function to pre-chew the dataset, and then saving the results back to
Tigris:

```python
biggest = -1
for shard_id, shard in enumerate(shards_for(bucket_name, dataset_name, storage_options)):
    ds = ds.map(prechew, batched=True)
    ds.save_to_disk(f"s3://{bucket_name}/model-ready/{model_name}/{dataset_name}/{shard_id}", storage_options=storage_options)
    biggest = shard_id

assert biggest != -1, "No examples were found in the dataset. Are you sure you imported the dataset correctly?"

tigris.write_text(f"/{bucket_name}/model-ready/{model_name}/{dataset_name}/info.json", json.dumps({"count": shard_id}))
```

The `prechew` function is a simple wrapper around the model's tokenization
formatting function:

```python
def prechew(examples):
    convos = examples["conversations"]
    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]
    return { "text" : texts, }
```

It takes an example, extracts the conversation text from it, and then uses the
model's tokenizer to convert that conversation text into the raw token form that
the model was trained on. In essence, it goes from this:

```json
{
  "conversations": [
    {
      "role": "user",
      "content": "Hello, computer, how many r's are in the word 'strawberry'?"
    },
    {
      "role": "assistant",
      "content": "There are three r's in the word 'strawberry'."
    }
  ]
}
```

And then turns them into something like this:

```text
<|im_start|>system
You are a helpful assistant that will make sure that the user gets the correct answer to their question. You are an expert at counting letters in words.
<|im_end|>
<|im_start|>user
Hello, computer, how many r's are in the word 'strawberry'?
<|im_end|>
<|im_start|>assistant
There are three r's in the word 'strawberry'.
<|im_end|>
```

This is what the model actually sees under the hood. Those `<|im_start|>` and
`<|im_end|>` tokens are special tokens that the model and inference runtime use
to know when a new message starts and ends.

Without these tokens the model would go catastrophically off the rails and start
spouting out nonsense that matches the word frequency patterns it was trained
on. This is because the model doesn't actually understand language, it just
knows how to predict the next word in a sequence of words. The `<|im_end|>`
token also has special meaning to the inference runtime because it means that
the model has finished generating a response and is ready to emit it to the
user.

### Actually training the model

Once the dataset is pre-chewed and everything else is ready in Tigris, the real
fun can begin. The model is loaded from Tigris:

```python
model = FastLanguageModel.from_pretrained(f"{home_folder}/tigris/models/{model_name}", ...)
```

And then we stack a new [LoRA](https://github.com/microsoft/LoRA) (Low-Rank
Adaptation) model on top of it. We use a LoRA adapter here because this requires
much less system resources to train than doing a full-blown finetuing run on the
dataset. When you use a LoRA, you freeze most of the weights in the original
model and only train a few weights that are stacked on top of the original
model. This allows you to train models on much larger datasets without having to
worry about running out of GPU memory or cloud credits.

There are tradeoffs in using LoRA adapters instead of doing a full fine-tuning
run, however in practice having LoRA models in the mix gives you more
flexibility because you can freeze and ship the base model to your datacenters
(or even cache it in a docker image) and then only have to worry about
distributing the LoRA adapter weights. The full model weights can be in the tens
of gigabytes, while the LoRA adapter weights are usually in the hundreds of
megabytes at most.

:::note

This doesn't always matter with Tigris because
[Tigris has no egress fees](https://www.tigrisdata.com/docs/pricing/), but
depending on the unique facts and circumstances of your deployment, it may be
ideal to have a smaller model to ship around on top of a known quantity. This
can also make updates easier in some enviroments. Consult your local SRE for
more life advice that may be helpful in your situation.

:::

```python
# Make a LoRA model stacked on top of the base model, this is what we train and
# save for later use.
model = FastLanguageModel.get_peft_model(model, ...) # other generic LoRA parameters here
```

Finally, the LoRA adapter is trained on each shard of the dataset for one epoch
and the resulting weights are saved to Tigris:

```python
# For every dataset shard:
for dataset in shards_for(bucket_name, model_name, dataset_name):
    # Load the Trainer instance for this shard
    trainer = SFTTrainer(
        model = model,
        tokenizer = tokenizer,
        train_dataset = dataset,
        dataset_text_field = "text", # The pre-chewed text field from before
        args = TrainingArguments(
            num_train_epochs = 1, # Number of times to iterate over the dataset when training
            ..., # other training arguments here, see dotrain.py for more details
        ),
        ...,
    )

    # Train the model for one epoch
    trainer_stats = trainer.train()
    print(trainer_stats)
```

Note that even though we're creating a new `SFTTrainer` instance each iteration
in the loop, we're not starting over from scratch each time. The `SFTTrainer`
will _mutate_ the LoRA adapter we're passing in the `model` parameter, so each
iteration in the loop is progressively training the model on the dataset.

The `trainer.train()` method returns a dictionary of statistics about the
training run, which we print out for debugging purposes. While the training run
is going on, it will emit information about the loss function, the learning
rate, and other things that are useful for debugging. Interpreting these
statistics is a bit of an art (less than reading tea leaves, but more than
reading tarot), but in general you want to see the loss function decreasing over
time and the learning rate decreasing over time. Consult your local data
scientist for more information and life advice as appropriate.

In this example, we save two products:

- The LoRA weights themselves, which aren't useful without the base model
- The LoRA weights fused with the base model, which is what you use for
  inference with tools like [vllm](https://docs.vllm.ai/en/latest/)

```python
# Save the LoRA model for later use
model.save_pretrained(f"{home_dir}/tigris/done/{model_name}/{dataset_name}/lora_model")
tokenizer.save_pretrained(f"{home_dir}/tigris/done/{model_name}/{dataset_name}/lora_model")

# Save the LoRA fused to the base model for inference with vllm
model.save_pretrained_merged(f"{home_dir}/tigris/done/{model_name}/{dataset_name}/fused", tokenizer, save_method="merged_16bit")
```

### Other things to have in mind

Each of the scripts that run in the instance are designed to be idempotent and
are intended to run in sequence, but `import-dataset.py` and `import-model.py`
can be run in parallel. This is because they don't depend on eachother and end
up feeding into inputs at the prechewing and training steps.

Training this LoRA adapter on a dataset of 100k examples takes about 15 minutes
including downloading the dataset, standardizing it, downloading the model,
saving it to Tigris, loading the model, pre-chewing the dataset, and training
the model. The instance will shut down automatically after 15 minutes of
inactivity to save you money.

In theory, this example can run on any nVidia GPU with at least 16 GB of vram
and in an environment with the FUSE character device `/dev/fuse` usable. If you
can't use FUSE for some reason, you will need to write custom python code to
interact with Tigris directly. This is left as an exercise for the reader. Hint:
use the `multiprocessing` module to parallelize the model weight loading and
saving steps.

:::note

The FUSE concept in the Linux kernel is different than the concept of fusing a
LoRA adapter to a base model. It is unfortunate that the two things are so
similar, but such is life.

:::

## Next steps

Once training is complete, you can use the model weights with
[`sky serve`](https://skypilot.readthedocs.io/en/latest/serving/sky-serve.html):

```text
sky serve up -n mymodel service.yaml --env-file .env
```

It will give you a URL that you can use to interact with your model:

```text
curl 3.84.15.251:30001/v1/chat/completions \
    -X POST \
    -d '{"model": "/home/ubuntu/tigris/done/Qwen/Qwen2.5-0.5B/mlabonne/FineTome-100k/fused", "messages": [{"role": "user", "content": "Who are you?"}]}' \
    -H 'Content-Type: application/json'
```

You can destroy this with `sky serve down`:

```text
sky serve down mymodel
```
